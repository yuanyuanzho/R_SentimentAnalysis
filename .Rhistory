f
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
train_
head(train)
head(train_)
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
train_
View(train_)
Boston
GermanCredit
data(GermanCredit)
scaled <- as.data.frame(lapply(loan_data, scale))
maxs <- as.numeric(apply(loan_data, 2, max) )
mins <- as.numeric(apply(loan_data, 2, min))
col <- c(2,3,8,9,12,13)
scaled <- as.data.frame(scale(loan_data[,col], center = mins, scale = maxs - mins))
col <- c(2,3,8,9,12,13)
maxs <- as.numeric(apply(loan_data[,col], 2, max) )
mins <- as.numeric(apply(loan_data[,col], 2, min))
scaled <- as.data.frame(scale(loan_data[,col], center = mins, scale = maxs - mins))
train_ <- scaled[index,]
head(train_)
test_ <- scaled[-index,]
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("Decision ~", paste(n[!n %in% "Decision"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
View(train_)
View(scaled)
install.packages("rJava")
library(rJava)
#install.packages("rvest")
#install.packages("SnowballC")
install.packages("tm")
install.packages("RWeka")
install.packages("rJava")
install.packages("RWekajars")
install.packages("tm.plugin.sentiment")
library(tm.plugin.sentiment)
library(rvest)
install.packages("xml2")
install.packages("xml2")
library(xml2)
library(rvest)
#count for number of pages.
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
write.csv(all_reviews, file = "allreviews.csv" )
getwd()
colnames(all_reviews)<- c("Reviews")
read.csv()
install.packages("tm")
library(tm)
install.packages("NLP")
install.packages("NLP")
library(NLP)
library(tm)
myCorpus <- Corpus(VectorSource(all_reviews))
inspect(myCorpus)
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove stopwords
# keep "r" by removing it from stopwords
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect the first three ``documents"
inspect(myCorpus[1:1000])
# stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
Sys.getenv()
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre")
remove.packages("rJava")
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre')
remove.packages("rJava")
library(rJava)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home/jre')
remove.packages("rJava")
install.packages("rJava")
library(rJava)
library(rJava)
remove.packages("rJava")
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre')
install.packages("rJava")
library(rJava)
remove.packages("rJava")
Sys.setenv(JAVA_HOME='/Users/eavy/Downloads/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre')
install.packages("rJava")
library(rJava)
library(rJava)
remove.packages("rJava")
Sys.setenv(JAVA_HOME='/Users/eavy/Downloads/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre')
Sys.setenv(JAVA_HOME='/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre')
install.packages("rJava")
install.packages("rJava")
library(rJava)
Sys.getenv("Path")
Sys.getenv()
Sys.getenv("JAVA_HOME")
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_144.jdk/Contents/Home/jre")
Sys.getenv("JAVA_HOME")
install.packages("rJava")
library(rJava)
install.packages("xlsx")
library(xlsx)
install.packages("xlsxjars")
install.packages("xlsxjars")
library(xlsxjars)
library(rJava)
library(xlsxjars)
library(xlsx)
install.packages("rvest")
install.packages("rJava")
install.packages("rJava")
install.packages(c("NLP", "openNLP", "RWeka", "qdap"))
install.packages("xml2")
library(xml2)
library(rvest)
library(rJava)
library(NLP)
library(openNLP)
library(RWeka)
#count for number of pages.
pages <- 100
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews <- rbind(all_reviews,review)
}
View(all_reviews)
#Tokenization
scan_tokenizer <- function(x)
scan(text = as.character(x), what = "character", quote = "",
quiet = TRUE)
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
#Stemming
wordStem(tokenized_reviews)
?wordStem
??wordStem
install.packages("wordStem")
view(Boston)
Boston
library(datasets)
Boston
library(MASS)
Boston
Diabetes
data()
library(lars)
diabetes
install.packages("lars")
install.packages("lars")
library(lars)
diabetes
Diabetes
write.csv(file = "/Users/eavy/Downloads/diabetes.csv")
write.csv(file = "/Users/eavy/Downloads/diabetes.csv", diabetes)
?lars
data(diabetes)
d  <- data(diabetes)
d
View(d)
library(lars)
data(diabetes)
diabetes
View(diabetes)
?corpus
myCorpus <- Corpus(VectorSource(all_reviews))
.
install.packages("tm")
install.packages("NLP")
library(tm)
library(NLP)
library(tm)
myCorpus <- Corpus(VectorSource(all_reviews))
?Corpus
?VectorSource
?Corpus
inspect(myCorpus)
myCorpus <- tm_map(myCorpus, tolower)
?tm_map
# trans to lowcase
myCorpus <- tm_map(myCorpus, tolower)
myCorpus
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove stopwords
# keep "r" by removing it from stopwords
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
install.packages("SnowballC")
library(SnowballC)
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect the first three ``documents"
inspect(myCorpus[1:1000])
# stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
myCorpus
scan_tokenizer <- function(x){
scan(text = as.character(x), what = "character", quote = "", quiet = TRUE)
}
token <- Token_Tokenizer(scan_tokenizer)
tokenized_reviews <- token(all_reviews)
str(tokenized_reviews)
inspect(myCorpus[1:100])
# inspect the first three ``documents"
inspect(myCorpus[1:10])
library(rvest)
#install.packages("RSelenium")
require(RSelenium)
library(xml2)
library(rvest)
#install.packages("RSelenium")
require(RSelenium)
remDr <- remoteDriver(remoteServerAddr = "localhost"
, port = 4445L
, browserName = "firefox"
)
#install.packages("httpuv")
#install.packages("tidytext")
#install.packages("hunspell")
#install.packages("tidyr")
#install.packages("dplyr")
#install.packages("RTextTools")
#install.packages("e1071")
library("httpuv")
install.packages("httpuv")
install.packages("tidytext")
install.packages("hunspell")
install.packages("tidyr")
install.packages("dplyr")
install.packages("RTextTools")
install.packages("e1071")
library("httpuv")
library("tidytext")
library("hunspell")
library("tidyr")
library("dplyr")
library("RTextTools")
library("e1071")
#  x = gsub("@\\w+", "", x) # remove at(@)
#  x = gsub("[[:punct:]]", "", x) # remove punctuation
#  x = gsub("[[:digit:]]", "", x) # remove numbers/Digits
#  x = gsub("http\\w+", "", x)  # remove links http
#  x = gsub("[ |\t]{2,}", "", x) # remove tabs
#  x = gsub("^ ", "", x)  # remove blank spaces at the beginning
#  x = gsub(" $", "", x) # remove blank spaces at the end
#  x = gsub("\n", "", x) # remove line break
#  return(x)
#}
for(j in 1:8000){
words <- hunspell_parse(words)
stems <- unlist(hunspell_stem(unlist(words)))
words <- as.data.frame(sort(table(stems), decreasing = TRUE))
words$stems <- as.character(words$stems)
join <- inner_join(as.data.frame(words), get_sentiments("bing"), by=c("stems" = "word"))
positive <- sum(join[join$sentiment == "positive",]$Freq)
negative <- sum(join[join$sentiment == "negative",]$Freq)
sentiment <- positive - negative
output[j,] <- c(j, sentiment, review_txt[j])
}
output <- read.csv('/Users/lucyy/Documents/AdvanceDataSci/Sentiment Analysis/output.csv')
getwd()
setwd
setwd("/Users/eavy/Downloads/7390/Assignment/Assignment8")
getwd
getwd()
pos <- scan('positive-words.txt', what ='character', comment.char = ";")
pos
match(myCorpusm pos)
match(myCorpusm, pos)
# stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
#count for number of pages.
pages <- 500
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews  <- rbind(all_reviews,review)
}
myCorpus <- Corpus(VectorSource(all_reviews))
inspect(myCorpus)
# trans to lowcase
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove stopwords
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect the first three ``documents"
inspect(myCorpus[1:10])
# stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
pos <- scan('positive-words.txt', what ='character', comment.char = ";")
neg <- scan('negative-words.txt', what ='character', comment.char = ";")
match(myCorpusm, pos)
match(myCorpus, pos)
myCorpus
all_reviews
tokenized_reviews
match(tokenized_reviews, pos)
# sentiment analysis
sum(!is.na(match(tokenized_reviews, pos)))
sum(!is.na(match(tokenized_reviews,neg)))
library(rvest)
library(xml2)
#install.packages("RSelenium")
require(RSelenium)
remDr <- remoteDriver(remoteServerAddr = "localhost"
, port = 4445L
, browserName = "firefox"
)
install.packages("RSelenium")
require(RSelenium)
library(RSelenium)
require(RSelenium)
remDr <- remoteDriver(remoteServerAddr = "localhost"
, port = 4445L
, browserName = "firefox"
)
remDr$open()
remDr$getStatus()
remDr$navigate("http://steamcommunity.com/app/582160/reviews?filterLanguage=english")
remDr$getCurrentUrl()
webElem <- remDr$findElement('xpath', "//*[@id='GetMoreContentBtn']/a")
#Try click get-more-content link to get more comments.It may show error when you do this, just try to run this code repeatly
for(i in 1:50){
Sys.sleep(1)
webElem$clickElement()
}
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "localhost"
, port = 4445L
, browserName = "firefox"
)
remDr$open()
all_reviews
all_reviews <- str_split(all_reviews, pattern="\\s+")
install.packages("stringr")
install.packages("stringr")
library(stringr)
all_reviews <- str_split(all_reviews, pattern="\\s+")
all_reviews
View(all_reviews)
str(all_reviews)
all_reviews <- gsub(pattern="\\w", replace=" ", all_reviews)
str(all_reviews)
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews  <- rbind(all_reviews,review)
}
a
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews  <- rbind(all_reviews,review)
}
all_reviews
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews  <- rbind(all_reviews,review)
}
library(rvest)
library(NLP)
library(tm)
library(SnowballC)
library(stringr)
#count for number of pages.
pages <- 500
all_reviews <- NULL
for(page_num in 1:pages){
#reading one page at a time
single_page <- paste0("https://www.amazon.com/All-New-Fire-TV-Stick-With-Alexa-Voice-Remote-Streaming-Media-Player/product-reviews/B00ZV9RDKK/ref=cm_cr_getr_d_show_all?ie=UTF8&reviewerType=all_reviews&showViewpoints=1&sortBy=helpful&pageNumber=", page_num)
#Converting to HTML
single_doc <- read_html(single_page)
#Parsing review text
review <-html_nodes(x= single_doc, css = ".review-text") %>%
html_text()
#Matrix conversion of data for columner form
review <- as.matrix(review)
#consolidating all reviews in one column
all_reviews  <- rbind(all_reviews,review)
}
all_reviews1 <- gsub(pattern="\\w", replace=" ", all_reviews)
all_reviews2 <- gsub(pattern = "\\d", replace=" ", all_reviews1)
all_reviews2
